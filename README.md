# ğŸ§™â€â™‚ï¸Math Wizard: SFT+DPO-Tuned gemma-3n


ğŸ¤— **Model Weights:** [gemma-3n-gek408-dpo](https://huggingface.co/Argobell/gemma-3n-gek408-dpo)  
ğŸ¤— **SFT Dataset:** [gek408](https://huggingface.co/datasets/Argobell/gek408)  
ğŸ¤— **DPO Dataset:** [gek408-dpo](https://huggingface.co/datasets/Argobell/gek408-dpo)

---

This repository contains a collection of scripts and resources for fine-tuning gemma-3n (using Unsloth). The process involves three main stages: **dataset processing**, **Supervised Fine-Tuning (SFT)**, and **Direct Preference Optimization (DPO)**.

## ğŸ“š Table of Contents

- [ğŸ§™â€â™‚ï¸Math Wizard: SFT+DPO-Tuned gemma-3n](#ï¸math-wizard-sftdpo-tuned-gemma-3n)
  - [ğŸ“š Table of Contents](#-table-of-contents)
  - [âš™ï¸ Installation](#ï¸-installation)
  - [âš¡ Core Framework: Unsloth](#-core-framework-unsloth)
  - [ğŸ“Š Dataset Preparation](#-dataset-preparation)
  - [ğŸ“ˆ SFT Supervised Fine-Tuning](#-sft-supervised-fine-tuning)
  - [ğŸ† DPO Direct Preference Optimization](#-dpo-direct-preference-optimization)
  - [ğŸš€ Quick Start](#-quick-start)
  - [ğŸ“‚ File Structure](#-file-structure)
  - [ğŸ”— Dependencies](#-dependencies)

## âš™ï¸ Installation

This project uses `uv` for package management. To create a virtual environment and install the dependencies, run the following commands:

```bash
# Create the virtual environment
uv venv --python=3.11

# Activate the environment
# On macOS/Linux:
source .venv/bin/activate
# On Windows:
.venv\Scripts\activate

# Install dependencies
uv sync
```

## âš¡ Core Framework: Unsloth

This project is powered by [Unsloth](https://github.com/unslothai/unsloth), a high-performance framework designed to significantly accelerate the fine-tuning of large language models.

**Key Advantages of Unsloth:**

*   **Faster Training:** Unsloth can make training up to 2x faster than standard Hugging Face methods.
*   **Memory Efficiency:** It reduces GPU memory usage by up to 70%, enabling you to fine-tune larger models or use bigger batch sizes on the same hardware.
*   **Ease of Use:** Unsloth seamlessly integrates with the Hugging Face ecosystem (like `transformers` and `peft`) and requires minimal code changes. It automates many optimizations, providing a smoother development experience.

By using Unsloth, we achieve faster iterations and more efficient use of computational resources.

## ğŸ“Š Dataset Preparation

Before training the model, the raw data needs to be preprocessed. The `src` directory contains the necessary scripts for this purpose.

The `@src/data_preprocess.py` script requires the `THU-KEG/MM_Math` dataset from Hugging Face. Please [download it here](https://huggingface.co/datasets/THU-KEG/MM_Math) and place it in the appropriate directory as instructed by the script.

To generate the SFT dataset, you must create a `.env` file in the project root and provide your `API_KEY` and `BASE_URL`.

![Preprocessing](resources/Preprocessing.png)

Below are the scripts and their example startup commands.

- **`src/data_preprocess.py`**: Cleans and preprocesses the raw data.
  ```bash
  python src/data_preprocess.py --image_folder path/to/your/images --json_path path/to/your/data.jsonl --output_path processed_data
  ```

- **`src/generate_sft_data.py`**: Generates conversational-style data required for the SFT stage. This script requires API access.
  ```bash
  # Process the training split
  python src/generate_sft_data.py --spilt train --input_path processed_data --output_path sft_data
  
  # Process the validation split
  python src/generate_sft_data.py --spilt val --input_path processed_data --output_path sft_data
  ```

- **`src/dpo_generator.py`**: Creates preference pairs (chosen & rejected) for the DPO stage.
  ```bash
  python src/dpo_generator.py --model_path path/to/your/sft_model --dataset_path sft_data --output_dir dpo_data --split validation
  ```

## ğŸ“ˆ SFT Supervised Fine-Tuning

In the SFT stage, the model is initially fine-tuned on the prepared dataset.

**SFT Dataset Construction:**

![SFT Dataset Construction](resources/SFT_Dataset_Construction.png)

**Start Training:**

Run the following script to start the SFT training:

```bash
bash train_sft.sh
```

This script will execute `src/sft_multi.py` with the required parameters.

**Training Results:**

The loss curve for the SFT stage is shown below:

![SFT Loss](resources/SFT%20Loss.png)

## ğŸ† DPO Direct Preference Optimization

The DPO stage further refines the SFT model using a preference dataset, aligning its outputs more closely with human preferences.

**DPO Dataset Construction:**

![DPO Dataset Construction](resources/DPO_Dataset_Construction.png)

**Start Training:**

Run the following script to start the DPO training:

```bash
bash train_dpo.sh
```

This script will execute `src/dpo_multi.py` with the required parameters.

**Training Results:**

The metrics for the DPO stage are as follows:

| DPO Loss | Reward Accuracy |
| :---: | :---: |
| ![DPO Loss](resources/DPO%20Loss.png) | ![Reward Accuracy](resources/Reward%20Accuracy.png) |
| **Reward Chosen** | **Reward Rejected** |
| ![Reward Chosen](resources/Reward%20Chosen.png) | ![Reward Rejected](resources/Reward%20Rejected.png) |
| **Reward Margin** |
| ![Reward Margin](resources/Reward%20Margin.png) |

## ğŸš€ Quick Start

The `examples` folder provides Jupyter Notebooks to help you get started quickly.

- `sft.ipynb`: Example for SFT model inference.
- `dpo.ipynb`: Example for DPO model inference.
- `inferenc.ipynb`: A complete example of the model inference pipeline.

## ğŸ“‚ File Structure

```
.
â”œâ”€â”€ .gitignore
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â”œâ”€â”€ train_dpo.sh
â”œâ”€â”€ train_sft.sh
â”œâ”€â”€ uv.lock
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ dpo.ipynb
â”‚   â”œâ”€â”€ inferenc.ipynb
â”‚   â””â”€â”€ sft.ipynb
â”œâ”€â”€ prompt/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ resources/
â”‚   â””â”€â”€ ...
â””â”€â”€ src/
    â”œâ”€â”€ data_preprocess.py
    â”œâ”€â”€ dpo_generator.py
    â”œâ”€â”€ dpo_multi.py
    â”œâ”€â”€ generate_sft_data.py
    â””â”€â”€ sft_multi.py
```

## ğŸ”— Dependencies

This project primarily relies on the following Python libraries:

- `torch`
- `transformers`
- `datasets`
- `peft`
- `trl`
- `bitsandbytes`
- `accelerate`
- `unsloth`
- `unsloth-zoo`

For a detailed list of dependencies, please refer to the `pyproject.toml` file.